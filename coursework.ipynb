{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian process Coursework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "This notebook owes a lot to the excellent resources made available by [gpflow](https://github.com/GPflow/GPflow), in addition to the [original assignment document](http://mlg.eng.cam.ac.uk/teaching/4f13/1920/cw/coursework1.pdf) by Carl Rasmussen. Many parts of it are directly copied from one or other of these resources to avoid re-inventing the wheel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Resources\n",
    "\n",
    "- I would highly recommend both David Mackay's [video lecture](http://videolectures.net/gpip06_mackay_gpb/) on Gaussian processes and the first chunk of Rich's [video lecture](https://www.youtube.com/watch?v=92-98SYOdlY) if you don't feel this GP stuff has clicked yet.\n",
    "- Carl Rasmussen's [Gaussian Processes for Machine Learning textbook](http://gaussianprocess.org/gpml/) remains the go-to textbook on the basics. Note that it was published in 2006, so necessarily doesn't cover more recent advances.\n",
    "- The [GPflow documentation](https://gpflow.readthedocs.io/en/master/?badge=master) is presumably something you will need to consult for this assignment. Likely of particular interest is the documentation on [kernels](https://gpflow.readthedocs.io/en/master/kernel_options.html), [vanilla GP regression](https://gpflow.readthedocs.io/en/master/model_options.html#gp-regression), [the vanilla GP regression notebook](https://nbviewer.jupyter.org/github/GPflow/GPflow/blob/develop/doc/source/notebooks/basics/regression.ipynb) (bits of which I consulted / copied when writing this assignment)\n",
    "- Also helpful is David Duvenaud's [kernel cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/index.html) if you want to understand some more about the properties of different kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry too much about this section, it should just run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "# The lines below are specific to the notebook format\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 6)\n",
    "plt = matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data for the assignment into variables `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import io\n",
    "import requests\n",
    "\n",
    "\n",
    "# To import a .mat file from a URL:\n",
    "r = requests.get('http://mlg.eng.cam.ac.uk/teaching/4f13/1920/cw/cw1a.mat')\n",
    "with io.BytesIO(r.content) as f:\n",
    "    data = scipy.io.loadmat(f)\n",
    "    x, y = data['x'], data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "\n",
    "Train a GP with a `gpflow.kernels.SquaredExponential` covariance function, with initial parameters `lengthscale=0.35`, `variance=1.0`, and likelihood variance `1.0`, by minimizing the negative log marginal likelihood. Show the 95% predictive error bars and draw some samples from the posterior. Comment on the predictive error bars and the optimized hyperparameters.\n",
    "\n",
    "(Hint: the GP regression notebook mentioned above will be helpful here, particularly when plotting the results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What value / extra information does visualising posterior samples provide in addition to the posterior marginals? \n",
    "(Hint: try repeating the exercise with `gpflow.kernels.Exponential`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that by initializing the hyperparameters differently, you can find a different local optimum for\n",
    "the hyperparameters. Try a range of values. Show the fit. Explain what the model is doing. Which\n",
    "fit is best, and why? How confident are you about this and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train instead a GP with a periodic covariance function. Show the fit. Comment on the behaviour\n",
    "of the error-bars, compared to your fit from a). Do you think the data generating mechanism (apart\n",
    "from the noise) was really strictly periodic? How confident are you about this, and why? Explain\n",
    "your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random (essentially) noise free functions evaluated at x = linspace(-5,5,200)â€™; from a GP with the following covariance function: {@covProd, {@covPeriodic, @covSEiso}}, with covari- ance hyperparameters hyp.cov = [-0.5 0 0 2 0]. In order to apply the Cholesky decomposition to the covariance matrix, you may have to add a small diagonal matrix, for example 1e-6*eye(200), why? Plot some sample functions. Explain the relationship between the properties of those random functions and the form of the covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download additional data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://mlg.eng.cam.ac.uk/teaching/4f13/1920/cw/cw1e.mat')\n",
    "with io.BytesIO(r.content) as f:\n",
    "    data = scipy.io.loadmat(f)\n",
    "    x_ml, y_ml = data['x'], data['y']"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
